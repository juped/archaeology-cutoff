Memory structure of Zeno (the loom):

1. Layout

You may know the typical Unix process layout, in which heap
grows upward from low memory, and stack grows downward
from high memory.  The general advantage of this layout
is zero fragmentation: heap and stack can grow as far 
toward each other as possible, until they meet.

This design is independent of the actual structures that grow
from both ends of a linear address space.  We might just as
well have two heaps growing toward each other, or two stacks.

The loom allocator is the latter - two stacks, growing toward 
each other.  A stack is actually a more powerful structure 
than a heap; if you have a stack and you want a heap, you can
always push one on top of the stack.  If you have a heap
and you want a (contiguous) stack, you are SOL.

We call these opposed stacks "beams."  A loom is two symmetric
beams, one growing up from the bottom of memory, the other
growing down from the top. The obvious constraint is that the
beams must never cross.  The unused area between them may be of
any length and at any position.

(The loom design is not Nock-specific.  Any functional language
with acyclic data structures can use it.  For instance, the same
approach could be used for functional processing of XML.)

All loom addresses are word (32-bit) addresses.  The loom
pointer format can address 2^28 words, or 1GB, which is 
adequate for most light-duty programming tasks.

A "ray" is a 29-bit word offset onto a beam.  If bit 29
is 0, the ray grows forward from 0 and is "west."  If bit
29 is 1, the ray grows backward from the end of the loom,
and is "east."  East and west address spaces overlap; any 
word in the loom has an east address and a west address.

(The loom mapping cannot be implemented in hardware by any
existing MMU I know of, but it is of course much simpler than a
page table, and relatively alacritous in software.  Twisted
pointers of various kinds are typical ingredients in any dynamic
language implementation.)

All u3 nouns are represented by the 32-bit word type "rat."
If bit 31 of a rat is 0, it is a "cat" - a direct atom.
Bits 0-30 of a cat are an unsigned 31-bit integer.

A rat can be a special value, u3_none - not a noun.  A
rat which is not u3_none is a "fox."

If bit 31 of a rat is 1, bits 0-28 are a ray which points
to an indirect noun - a "dog."  If bit 30 is 0, the dog is
a "pom" - an indirect cell.  If bit 30 is 1, the noun is a
"pug" - an indirect atom.

Bit 29 on a dog is set iff the dog has special information
behind the pointer, including but not limited to allocation
header (block and reference count).

Although poms and pugs cannot be referenced through C pointers,
because a ray is not a C pointer and requires specialized address
arithmetic, their shape is described by these simple structures:

      struct u3_cell {
        c3_w   mug_w;
        u3_ray hed_ray;
        u3_ray tel_ray;
      };
      struct u3_atom {
        c3_w mug_w;
        c3_w len_w;
        c3_w buf_w[0];
      };

"c3_w" and "u3_ray" are both uint32_t.  "buf_w" is really an
array of length "len_w," of course.

"mug_w" is a 31-bit insecure hash which matches the Watt (mug)
gate.  The mug is computed lazily, as we need it - 0 is an
out-of-band value.  It is useful not just in associative and
balanced trees, but also in simple functions like equality.  If
two dogs have the same mug, they may be different nouns; if they
have different mugs, they must be different nouns.

We also borrow the mug field to maintain dag structure when
copying nouns.  Why copy a noun?  See below.

2. Allocation (simple)

Now we consider the loom's allocation algorithm, the "Seuss
system."  The Seuss system manages memory with three pointers:
"hat", "cap", and "mat".

"hat" and "cap", rays, are the tops of two opposing stack
allocators, rays on opposite beams.  Either the hat is east
and the cap is west, or vice versa.

"mat" is a ray on the same beam as the cap, at or below it.

The loom is hence divided into three parts:

0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->.....box.......---pad----**can**........box.........<-|
|                                                        | 
west                                                  east

The "pad" is free memory.  The "box" is stable memory.  The
"can" is temporary memory.  Again, the loom is symmetric.  The
can is always on one side of the pad, but that side can be east
(as shown above), or west:

0               mat       cap     hat                  1GB
|                |         |       |                     | 
|->.....box.......***can****--pad--........box.........<-|
|                                                        | 
west                                                  east

The loom imposes a seniority restriction on all pointers.  Box is
senior to can.  A reference stored in the can may point into the
box, but a reference stored in the box cannot point into the can.

Since the box cannot point to the can, the can may be entirely
destroyed without any corruption of the box.  In general, the can
is used for temporary allocation - ie, future garbage.

So the programmer in this context has two allocation choices.
She can allocate garbage on the cap, or product on the hat.
Rather than the normal model of stack and heap, she has two stack
pointers - cap and hat - which grow toward each other.

In general, any function F is expected to produce its product as a
fragmentation-free block of nouns, allocated in the box (on the
hat), with pointers to older data in the box, but not of course
the can.  The can is expected to be reset after the call.  Thus
F is expected, as part of the calling convention, to pick up its
own garbage.

Two stacks seems like a lot of choices for the programmer, every
time she allocates a cell.  Actually, though, this choice can in
normal circumstances be made by the interpreter - because the
only memory management problem in acyclic functional programming
is composition.

When computing F(x), Zeno allocates product on the hat.  When
computing F(G(x)), Zeno allocates product of G(x) on the cap, and
product of F(x) on the hat.  If F(x) uses unmodified product of
G(x), these must be copied from cap to hat.  For the product of
G(x) is, by definition, garbage.

And when computing F(G(H(x)))?  Sadly, the calculations that
produce garbage have a nasty tendency to allocate temporary nouns
of their own.  If we had an infinite number of beams, we'd be
set.  But we only have two.

Zeno handles the H(x) problem by reversing the beams.  Let's work
through a simple example:

As we start:

0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............----------*******....................<-|
|                                                        | 
west                                                  east

"." is permanent memory, the box; "*" is temporary memory, the
can; "-" is free memory, the pad.

Our goal is to allocate the product of F as ".", by pushing
the hat a couple of columns east.

First, we depart to compute G.  The product of G is "!", and is
allocated on the cap.  Not that G is hardcoded for this role, of
course - it allocates its product on the hat, like any function.

                old        old    old
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............--------!!*******....................<-|
|                |        |                              | 
west            mat      hat                          east
                cap
                now      now

Thus, so long as the VM exhibits east/west independence,
we can compute G with the same code that computed F, on the
opposite beam.  And if G depends on another function, H?

                old        old    old
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............????----!!*******....................<-|
|                |   |    |                              | 
west            mat cap  hat                          east
                now now  now

The garbage of G - which is the product of H - is "?".  How do we
get H to allocate on the cap?  By reversing the beams again.
This solution can be iterated indefinitely - until, of course,
the cap and hat collide.  At this point you are out of memory.

We complete the calculation of G by retreating:

                old        old    old 
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............--------!!*******....................<-|
|                |        |        |                     | 
|               hat      cap      mat                 east
west            now      now      now                 

We have now computed G and placed its product on the cap.  So we
are ready to compute F, placing its product on the hat.  The
product of F is "~":

                old        old    old 
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............~~------!!*******....................<-|
|                  |      |        |                     | 
west              hat    cap      mat                 east
                  now    now      now                  

We then retract the cap, freeing the product of G:

                old        old    old 
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............~~--------*******....................<-|
|                  |        |      |                     | 
west              hat      cap    mat                 east
                  now      now    now                  

Thus we have pushed the product of F, without any garbage,
on the hat.  The algorithm is optimal, except for the cost of
copying temporary results - which varies from 0 to prohibitive.

One case in which the algorithm above needs improvement is
tail-call compaction.  In a tail-call loop, F(x) resolves to
F(G(x)) - which resolves to F(G(G(x))), etc, etc.  

The loom knows the final destination at all times and never needs
to copy the result - but it also accumulates a pile of
intermediate results, G(x), G(G(x)), etc, on the cap.  Where
"!" is G(x), "?" is G(G(x)), '#" is G(G(G(x))):

                old        old    old 
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............----##??!!*******....................<-|
|                |    |            |                     | 
|               hat  cap          mat                 east
west            now  now          now                 

This is obviously craptastic.  Can anything be done about it?
That depends on one thing - whether G(G(x)) references G(x).  If
it does (for example, when inverting a list), no compaction is
possible - there is nothing to free.

But if G(G(x)) does not reference G(x), we can simply slide
G(G(x)) down the cap over its parent.  In general, tail calls can
be computed in fixed space if the new tail context does not
reference the old tail context.

How can we determine this?  Well, we could analyze the formula.
In a compilation context, this is probably what we'd do.  In an
interpreter, however, there is no room for static analysis.

Therefore, the only way to determine whether G(G(x)) references
G(x) is a brute-force nano-garbage collection.  We simply chase
any pointers in G(G(x)) and see if they fall into G(x).  If not,
we slide "?" down over "!".  This is called "tamping."

Tamping is a effective optimization because its cost is
proportional to the size of G(G(x)).  This may be an arbitrarily
large and complex noun, but we do not have to chase pointers
outside G(G(x)).  "?" cannot address "!" through either "." or
"*", because "." and "*" cannot point to "!" due to seniority.

A tamped loom makes an excellent top-level default.  For many
algorithms, it is entirely impractical.  But, if the implementor
of the algorithm knows this, it is easy to request special
treatment.  The pad is an unfragmented block of memory - the
appropriate input for any memory-management algorithm.

The Watt programmer, optimizing for Zeno, does so by using hints
(Nock 11) to select one of three memory disciplines: %flee,
%stay, and %keep.  The above is the %flee discipline, which 
is the default.  In general, the memory discipline defines what 
Zeno does where, under %flee, it would depart.
 
The %stay discipline never departs.  Instead, it uses the cap as
a conventional stack, and the hat as a conventional heap.  Only
control information is allocated on the cap.  All product nouns -
F(x), G(x) and H(x) - are allocated on the hat:

                old        old    old 
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............??!!~~----*******....................<-|
|                      |    |      |                     | 
west                  hat  cap    mat                 east
                      now  now    now                  

What do we do with G(x) and H(x)?  We leak the motherfsckers.
But at least there is no copying.

This is remedied by the %keep discipline.  Remember that
disciplines affect one thing: departure policy.  In the %flee
discipline, we depart; in the %stay discipline, we remain.  

In %keep, we depart - but install a refcount allocator.  In the
departing frame, this upgrades the hat to a reference-counted
heap.  It also sets the memory discipline to %stay.  Thus, in the
departing frame, we do not leak but rather free.

Since we are refcounting in the departed frame, not the current
frame, the result still needs to be copied back if used.  It is
good to copy out of an allocator, because the allocator doubles
the size of a cell.

Therefore, if we compute F under %keep, we observe, in the
departed context,

                old        old    old
0               hat        cap    mat                  1GB
|                |          |      |                     | 
|->...............-----?=!?&*******....................<-|
|                |     |   |                             | 
west            mat   hat rut                           east
                cap
                now   now now  

"&" is the free list; "=" is a free block.

Using the allocator adds a fourth memory pointer, "rut", to the
Seuss system.  The rut sets (a) the location of the free list
and (b) the boundaries of reference control.  If the rut is set,
before advancing the hat, zeno will try to allocate out of the
free list.

Since nouns are acyclic, reference counting is always effective.
Why bother with this loom business, then?  Why not just a simple
reference-counting allocator for all of memory?

One of the big performance problems with refcounting is its
tendency to absolutely screw the hell out of your cache.  Address
space can be backed by anything - from on-chip cache to a tape
drive.  Many programs pass around references to memory they use
much more rarely, and when they use this memory it is only to
read them.  If the entire space is in one reference pool, this
will generate a shower of *writes* to deep data structures that
are essentially static.

Reference counting in the loom is *local*.  References only need
to be tracked for nouns between rut and hat.  Even if these nouns
reference reference-counted nouns elsewhere in the loom, counts
do not need to be updated for any noun between 0 and hat, or rut
and 1GB.
